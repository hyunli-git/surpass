// src/app/skill-practice/speaking/page.tsx

"use client";

import { useState, useRef, useEffect } from 'react';
import { supabase } from '@/utils/supabaseClient';

// ì»´í¬ë„ŒíŠ¸ì˜ ì—¬ëŸ¬ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
type PracticeStatus = 'loading' | 'ready' | 'preparing' | 'speaking' | 'finished';

interface SpeakingQuestion {
  part: number;
  topic: string;
  prompt: string;
}

export default function SpeakingPracticePage() {
  const [status, setStatus] = useState<PracticeStatus>('loading');
  const [question, setQuestion] = useState<SpeakingQuestion | null>(null);
  const [audioURL, setAudioURL] = useState('');
  const [timer, setTimer] = useState(60); // íƒ€ì´ë¨¸ ì´ˆê¸°ê°’ì€ 60ì´ˆ
  
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const timerIntervalRef = useRef<NodeJS.Timeout | null>(null);

  // í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ (OpenAI TTS ì‚¬ìš©)
  const speak = async (text: string) => {
    try {
      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ 
          text, 
          voice: 'nova' // ì—¬ì„± ëª©ì†Œë¦¬, ìì—°ìŠ¤ëŸ¬ìš´ í†¤
        }),
      });

      if (!response.ok) {
        console.warn('TTS API failed, falling back to browser TTS');
        fallbackSpeak(text);
        return;
      }

      const audioBuffer = await response.arrayBuffer();
      const audioBlob = new Blob([audioBuffer], { type: 'audio/mpeg' });
      const audioUrl = URL.createObjectURL(audioBlob);
      
      const audio = new Audio(audioUrl);
      audio.play();

      // Clean up the object URL after playing
      audio.onended = () => {
        URL.revokeObjectURL(audioUrl);
      };

    } catch (error) {
      console.error('TTS Error:', error);
      fallbackSpeak(text);
    }
  };

  // ë¸Œë¼ìš°ì € ê¸°ë³¸ TTS (ë°±ì—…ìš©)
  const fallbackSpeak = (text: string) => {
    if (!window.speechSynthesis) {
      console.warn("Browser does not support Speech Synthesis.");
      return;
    }
    window.speechSynthesis.cancel();
    
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'en-US';
    utterance.rate = 0.9;
    window.speechSynthesis.speak(utterance);
  };

  // í˜ì´ì§€ ë¡œë“œ ì‹œ ì§ˆë¬¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
  useEffect(() => {
    const fetchQuestion = async () => {
      const { data, error } = await supabase
        .from('practice_questions')
        .select('*')
        .eq('skill_type', 'Speaking')
        .eq('part', 2)
        .limit(1)
        .single();

      if (error || !data) {
        alert('ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.');
      } else {
        setQuestion(data);
        setStatus('ready'); // ë°”ë¡œ preparingìœ¼ë¡œ ê°€ì§€ ì•Šê³  ready ìƒíƒœë¡œ
      }
    };
    fetchQuestion();

    return () => {
      window.speechSynthesis.cancel();
    };
  }, []);

  // ìƒíƒœ(status)ê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ íƒ€ì´ë¨¸ ë¡œì§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
  useEffect(() => {
    if (timerIntervalRef.current) clearInterval(timerIntervalRef.current);

    if (status === 'preparing') {
      setTimer(60); // 1ë¶„ ì¤€ë¹„ ì‹œê°„
      timerIntervalRef.current = setInterval(() => {
        setTimer(prev => {
          if (prev <= 1) {
            clearInterval(timerIntervalRef.current!);
            speak("Your preparation time is over. Please start speaking now.");
            startRecording();
            return 0;
          }
          return prev - 1;
        });
      }, 1000);
    } else if (status === 'speaking') {
      setTimer(120); // 2ë¶„ ë‹µë³€ ì‹œê°„
      timerIntervalRef.current = setInterval(() => {
        setTimer(prev => {
          if (prev <= 1) {
            clearInterval(timerIntervalRef.current!);
            stopRecording();
            return 0;
          }
          return prev - 1;
        });
      }, 1000);
    }

    return () => {
      if (timerIntervalRef.current) clearInterval(timerIntervalRef.current);
    };
  }, [status]);

  const startRecording = async () => {
    setStatus('speaking');
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      const audioChunks: Blob[] = [];
      mediaRecorder.ondataavailable = (event) => audioChunks.push(event.data);
      mediaRecorder.onstop = () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        const url = URL.createObjectURL(audioBlob);
        setAudioURL(url);
        setStatus('finished');
        speak("Your speaking time is over.");
      };
      mediaRecorder.start();
    } catch (err) {
      alert('ë§ˆì´í¬ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
      setStatus('preparing');
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.stop();
    }
  };

  const handleGetFeedback = () => alert("AI ë¶„ì„ ê¸°ëŠ¥ì€ ê³§ ì¶”ê°€ë  ì˜ˆì •ì…ë‹ˆë‹¤!");

  const startPractice = () => {
    if (!question) return;
    
    const textToSpeak = `Part ${question.part}. The topic is ${question.topic}. Please, describe a book that you enjoyed reading. You have one minute to prepare.`;
    speak(textToSpeak);
    setStatus('preparing');
  };

  const formatTime = (seconds: number) => {
    const minutes = Math.floor(seconds / 60).toString().padStart(2, '0');
    const secs = (seconds % 60).toString().padStart(2, '0');
    return `${minutes}:${secs}`;
  };

  const renderStatusUI = () => {
    switch (status) {
      case 'ready':
        return (
          <div className="ready-state">
            <div className="practice-instructions">
              <h4>ğŸ¤ Speaking Practice Instructions</h4>
              <ul>
                <li>Make sure your microphone is working</li>
                <li>Find a quiet place to practice</li>
                <li>You&apos;ll have 1 minute to prepare, then 2 minutes to speak</li>
                <li>The system will provide audio instructions</li>
              </ul>
            </div>
            <button 
              onClick={startPractice} 
              className="btn btn-primary"
              style={{ fontSize: '1.125rem', padding: 'var(--space-md) var(--space-xl)' }}
            >
              ğŸ¯ Start Speaking Practice
            </button>
          </div>
        );
      case 'preparing':
        return <div>ì¤€ë¹„ ì‹œê°„: {formatTime(timer)}</div>;
      case 'speaking':
        return <div style={{color: 'var(--accent-red)'}}>ë‹µë³€ ì‹œê°„: {formatTime(timer)} (ë…¹ìŒ ì¤‘...)</div>;
      case 'finished':
        return <div>ì‹œí—˜ ì¢…ë£Œ</div>;
      default:
        return <div>ì§ˆë¬¸ ë¡œë”© ì¤‘...</div>;
    }
  };

  return (
    <div className="container" style={{ maxWidth: '700px', margin: '50px auto' }}>
      <h1>IELTS Speaking Practice (Part 2)</h1>
      
      <div className="question-box" style={{ marginBottom: 'var(--space-xl)' }}>
        {question ? (
          <>
            <h3>{question.topic}</h3>
            <pre style={{ whiteSpace: 'pre-wrap', fontFamily: 'inherit', margin: '0', color: 'var(--text-secondary)' }}>
              {question.prompt}
            </pre>
          </>
        ) : (
          <p>Loading question...</p>
        )}
      </div>

      <div className="question-box">
        <h3>Your Response</h3>
        <div style={{ padding: '20px 0', fontSize: '1.5rem', fontWeight: '600' }}>
          {renderStatusUI()}
        </div>

        {status === 'finished' && audioURL && (
          <div>
            <h4>Listen to your recording:</h4>
            <audio src={audioURL} controls style={{ width: '100%' }} />
            <button onClick={handleGetFeedback} className="btn btn-success" style={{ marginTop: '20px', width: '100%' }}>
              Get AI Feedback
            </button>
          </div>
        )}
      </div>
    </div>
  );
}